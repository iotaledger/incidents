# Incident summary - Mainnet 12/11/2020

This post serves as a summary of the IOTA Mainnet incident that occurred on November 12, 2020 and November 13, 2020.

## Incident summary

The incident occured at 19:45 UTC on November 12, 2020 and was fully resolved by 12:37 UTC on November 13, 2020. During the period, no more milestones were issued on the network.

## Leadup

No individual change in the node software, or any other components of the network, led to this event. The event was triggerred by an increased load on the network during a community spam event, during which the network reached over 1000 TPS. At the last stage of the event, the infrastructure hosting some of the critical components of the network reached its IOPS processing limits.

## Impact

Between 19:45 UTC on November 12, 2020 and 12:37 UTC on November 13, 2020, users were unable to confirm transactions on the IOTA Mainnet network.

## Timeline

**12/11/2020**
1. 19:10 UTC - Spam event started on the IOTA mainnet. 
2. 19:10 UTC - The devops team has been monitoring the situation from the start of the event.
3. 19:17 UTC - Network components started reaching a large number of IOPS.
4. 19:45:00 UTC - Infrastructure started having issues processing all the transactions on the network.
5. 19:45:49 UTC - Milestone 2272659 was issued. Network nodes successfully solidified on this milestone. Subsequent milestones 2272660, 2272661 were then issued by the coordinator. No node was able to solidify on these milestones.
6. 20:03:39 UTC - The network coordinator is now offline. 
7. 20:05:00 UTC - The network and development teams are assessing the situation.

**13/11/2020**

8. 01:30 UTC - A fix for the situation is devised that requires an update to the node software. 
9. 02:50 UTC - The devops team has finished testing the fix in an isolated manner.
10. 08:00 UTC - A second round of review for the fix.
11. 12:37 UTC - A new version of Hornet is released and the infrastructure is upgraded to the new version of the node software.

## Root cause

The infrastructure and by that subsequently the deployed software were not able to process all the transactions coming into the network. This resulted in the coordinator emitting milestones for transactions that were not persisted due to the aforementioned IO bottleneck. Due to the load on the network as a whole, the Coordinator was unable to gossip the necessary transactions to solidify the emitted milestones before it was halted, leading to the entire network not being able to solidify.

## Lessons learned

- The IOPS limits on the infrastructure around the coordinator need to be improved.
- There needs to be a redundancy mechanism in place that ensures the retention of all transactions sent to and broadcasted from the coordinator. 
- The coordinator needs to react to the network state and pause milestone issuance automatically in situations that approach the limit of hardware resources.

## Recurrence

Actions mentioned in the lessons learned section have been taken - infrastructure change, and plans to minimize the probability of this event recurring.

This incident and the preceding spam event have helped us to refine our response protocols. We would like to thank our community for their patience. A huge thank you also goes to our engineering team, and the Hornet team, who worked late into the night until a fix was devised and ready to be tested.
 
